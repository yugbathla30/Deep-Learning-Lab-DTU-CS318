{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1485ad9a",
   "metadata": {},
   "source": [
    "# Assignment 1: Mathematical Foundations of Deep Learning\n",
    "\n",
    "**Objective:** To implement and understand the linear algebra, calculus, and probability concepts that form the mathematical foundation of deep learning, including vectors, matrices, gradients, entropy, and KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: NumPy Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4884e5",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73609b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.4.1\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128db1e",
   "metadata": {},
   "source": [
    "### 1(i) Vector Operations with NumPy\n",
    "Create two vectors and compute:\n",
    "- Dot product\n",
    "- L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3032cd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector v1: [1 2 3 4]\n",
      "Vector v2: [5 6 7 8]\n",
      "\n",
      "Dot product (v1 · v2): 70\n",
      "\n",
      "L2 norm of v1: 5.477225575051661\n",
      "L2 norm of v2: 13.19090595827292\n"
     ]
    }
   ],
   "source": [
    "# Create two vectors\n",
    "v1 = np.array([1, 2, 3, 4])\n",
    "v2 = np.array([5, 6, 7, 8])\n",
    "\n",
    "print(\"Vector v1:\", v1)\n",
    "print(\"Vector v2:\", v2)\n",
    "\n",
    "# Dot product\n",
    "dot_product = np.dot(v1, v2)\n",
    "print(\"\\nDot product (v1 · v2):\", dot_product)\n",
    "\n",
    "# L2 norm (Euclidean norm)\n",
    "l2_norm_v1 = np.linalg.norm(v1)\n",
    "l2_norm_v2 = np.linalg.norm(v2)\n",
    "print(\"\\nL2 norm of v1:\", l2_norm_v1)\n",
    "print(\"L2 norm of v2:\", l2_norm_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75657bb",
   "metadata": {},
   "source": [
    "### 1(ii) Matrix Operations with NumPy\n",
    "Create two matrices and compute:\n",
    "- Matrix multiplication\n",
    "- Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dabf6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Matrix B:\n",
      "[[5 6]\n",
      " [7 8]]\n",
      "\n",
      "Matrix multiplication (A × B):\n",
      "[[19 22]\n",
      " [43 50]]\n",
      "\n",
      "Eigenvalues of A: [-0.37228132  5.37228132]\n",
      "Eigenvectors of A:\n",
      "[[-0.82456484 -0.41597356]\n",
      " [ 0.56576746 -0.90937671]]\n",
      "\n",
      "Eigenvalues of B: [-0.15206735 13.15206735]\n"
     ]
    }
   ],
   "source": [
    "# Create two matrices\n",
    "A = np.array([[1, 2], \n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6], \n",
    "              [7, 8]])\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "# Matrix multiplication\n",
    "matrix_product = np.matmul(A, B)  # or A @ B\n",
    "print(\"\\nMatrix multiplication (A × B):\")\n",
    "print(matrix_product)\n",
    "\n",
    "# Eigenvalues (for square matrix A)\n",
    "eigenvalues_A, eigenvectors_A = np.linalg.eig(A)\n",
    "print(\"\\nEigenvalues of A:\", eigenvalues_A)\n",
    "print(\"Eigenvectors of A:\")\n",
    "print(eigenvectors_A)\n",
    "\n",
    "# Eigenvalues for matrix B\n",
    "eigenvalues_B, eigenvectors_B = np.linalg.eig(B)\n",
    "print(\"\\nEigenvalues of B:\", eigenvalues_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e94a82",
   "metadata": {},
   "source": [
    "### 1(iii) Partial Derivatives and Gradient Computation\n",
    "For the function $f(x, y) = x^2 + 3y^2 + 2xy$\n",
    "\n",
    "Compute:\n",
    "- Partial derivative with respect to x: $\\frac{\\partial f}{\\partial x} = 2x + 2y$\n",
    "- Partial derivative with respect to y: $\\frac{\\partial f}{\\partial y} = 6y + 2x$\n",
    "- Gradient vector: $\\nabla f = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc753fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function f(x,y) = x² + 3y² + 2xy\n",
      "\n",
      "At point (x=1, y=2):\n",
      "f(1, 2) = 17\n",
      "\n",
      "Partial derivative ∂f/∂x = 2x + 2y = 6\n",
      "Partial derivative ∂f/∂y = 6y + 2x = 14\n",
      "\n",
      "Gradient vector ∇f = [ 6 14]\n"
     ]
    }
   ],
   "source": [
    "# Define the function f(x, y) = x^2 + 3y^2 + 2xy\n",
    "def f(x, y):\n",
    "    return x**2 + 3*y**2 + 2*x*y\n",
    "\n",
    "# Partial derivatives (analytically derived)\n",
    "def partial_f_x(x, y):\n",
    "    \"\"\"Partial derivative of f with respect to x: ∂f/∂x = 2x + 2y\"\"\"\n",
    "    return 2*x + 2*y\n",
    "\n",
    "def partial_f_y(x, y):\n",
    "    \"\"\"Partial derivative of f with respect to y: ∂f/∂y = 6y + 2x\"\"\"\n",
    "    return 6*y + 2*x\n",
    "\n",
    "# Gradient vector\n",
    "def gradient_f(x, y):\n",
    "    \"\"\"Gradient vector: ∇f = (∂f/∂x, ∂f/∂y)\"\"\"\n",
    "    return np.array([partial_f_x(x, y), partial_f_y(x, y)])\n",
    "\n",
    "# Example: Compute at point (x=1, y=2)\n",
    "x, y = 1, 2\n",
    "\n",
    "print(f\"Function f(x,y) = x² + 3y² + 2xy\")\n",
    "print(f\"\\nAt point (x={x}, y={y}):\")\n",
    "print(f\"f({x}, {y}) = {f(x, y)}\")\n",
    "print(f\"\\nPartial derivative ∂f/∂x = 2x + 2y = {partial_f_x(x, y)}\")\n",
    "print(f\"Partial derivative ∂f/∂y = 6y + 2x = {partial_f_y(x, y)}\")\n",
    "print(f\"\\nGradient vector ∇f = {gradient_f(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b01bbc",
   "metadata": {},
   "source": [
    "### 1(iv) Entropy Calculation\n",
    "Given a discrete probability distribution, compute Shannon Entropy:\n",
    "\n",
    "$$H(P) = -\\sum_{i} P(x_i) \\log_2 P(x_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2129f9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution P: [0.25 0.25 0.25 0.25]\n",
      "Sum of probabilities: 1.0\n",
      "\n",
      "Entropy H(P) = 2.0000 bits\n",
      "\n",
      "Probability distribution Q: [0.5   0.25  0.125 0.125]\n",
      "Entropy H(Q) = 1.7500 bits\n",
      "\n",
      "Note: Uniform distribution has higher entropy (2.0000) than non-uniform (1.7500)\n"
     ]
    }
   ],
   "source": [
    "# Define a discrete probability distribution\n",
    "P = np.array([0.25, 0.25, 0.25, 0.25])  # Uniform distribution\n",
    "print(\"Probability distribution P:\", P)\n",
    "print(\"Sum of probabilities:\", np.sum(P))\n",
    "\n",
    "# Entropy function: H(P) = -Σ P(x) * log2(P(x))\n",
    "def entropy(p):\n",
    "    \"\"\"Compute Shannon entropy of a probability distribution\"\"\"\n",
    "    # Filter out zero probabilities to avoid log(0)\n",
    "    p = p[p > 0]\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "entropy_P = entropy(P)\n",
    "print(f\"\\nEntropy H(P) = {entropy_P:.4f} bits\")\n",
    "\n",
    "# Example with a non-uniform distribution\n",
    "Q = np.array([0.5, 0.25, 0.125, 0.125])\n",
    "print(f\"\\nProbability distribution Q: {Q}\")\n",
    "entropy_Q = entropy(Q)\n",
    "print(f\"Entropy H(Q) = {entropy_Q:.4f} bits\")\n",
    "\n",
    "# Note: Uniform distribution has maximum entropy\n",
    "print(f\"\\nNote: Uniform distribution has higher entropy ({entropy_P:.4f}) than non-uniform ({entropy_Q:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeef998",
   "metadata": {},
   "source": [
    "### 1(v) Kullback-Leibler (KL) Divergence\n",
    "Given two probability distributions P and Q, compute KL divergence:\n",
    "\n",
    "$$D_{KL}(P \\| Q) = \\sum_{i} P(x_i) \\log \\frac{P(x_i)}{Q(x_i)}$$\n",
    "\n",
    "KL divergence measures how one probability distribution diverges from a reference distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce07ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution P: [0.4 0.3 0.2 0.1]\n",
      "Distribution Q: [0.25 0.25 0.25 0.25]\n",
      "\n",
      "KL Divergence D_KL(P || Q) = 0.1064\n",
      "KL Divergence D_KL(Q || P) = 0.1218\n",
      "\n",
      "Note: KL divergence is NOT symmetric: D_KL(P||Q) ≠ D_KL(Q||P)\n"
     ]
    }
   ],
   "source": [
    "# Define two probability distributions\n",
    "P = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "Q = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "print(\"Distribution P:\", P)\n",
    "print(\"Distribution Q:\", Q)\n",
    "\n",
    "# KL Divergence: D_KL(P || Q) = Σ P(x) * log(P(x) / Q(x))\n",
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute KL divergence D_KL(P || Q)\"\"\"\n",
    "    # Filter out zero probabilities in P\n",
    "    mask = p > 0\n",
    "    p = p[mask]\n",
    "    q = q[mask]\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "kl_div_PQ = kl_divergence(P, Q)\n",
    "print(f\"\\nKL Divergence D_KL(P || Q) = {kl_div_PQ:.4f}\")\n",
    "\n",
    "# KL divergence is not symmetric\n",
    "kl_div_QP = kl_divergence(Q, P)\n",
    "print(f\"KL Divergence D_KL(Q || P) = {kl_div_QP:.4f}\")\n",
    "\n",
    "print(f\"\\nNote: KL divergence is NOT symmetric: D_KL(P||Q) ≠ D_KL(Q||P)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eded65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: PyTorch Implementations\n",
    "\n",
    "Understanding and implementing tensor operations used in deep learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a534e48",
   "metadata": {},
   "source": [
    "### 2(i) Creating Tensors with PyTorch\n",
    "Create tensors of different dimensions:\n",
    "- Scalar (0D tensor)\n",
    "- Vector (1D tensor)\n",
    "- Matrix (2D tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b481f560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar (0D tensor): tensor(5.)\n",
      "Shape: torch.Size([])\n",
      "Dimension: 0\n",
      "\n",
      "Vector (1D tensor): tensor([1., 2., 3., 4.])\n",
      "Shape: torch.Size([4])\n",
      "Dimension: 1\n",
      "\n",
      "Matrix (2D tensor):\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Shape: torch.Size([2, 3])\n",
      "Dimension: 2\n",
      "\n",
      "3D Tensor:\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "Shape: torch.Size([2, 2, 2])\n",
      "Dimension: 3\n"
     ]
    }
   ],
   "source": [
    "# Scalar (0D tensor)\n",
    "scalar = torch.tensor(5.0)\n",
    "print(\"Scalar (0D tensor):\", scalar)\n",
    "print(\"Shape:\", scalar.shape)\n",
    "print(\"Dimension:\", scalar.ndim)\n",
    "\n",
    "# Vector (1D tensor)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(\"\\nVector (1D tensor):\", vector)\n",
    "print(\"Shape:\", vector.shape)\n",
    "print(\"Dimension:\", vector.ndim)\n",
    "\n",
    "# Matrix (2D tensor)\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                       [4.0, 5.0, 6.0]])\n",
    "print(\"\\nMatrix (2D tensor):\")\n",
    "print(matrix)\n",
    "print(\"Shape:\", matrix.shape)\n",
    "print(\"Dimension:\", matrix.ndim)\n",
    "\n",
    "# 3D Tensor (for completeness)\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]],\n",
    "                          [[5, 6], [7, 8]]])\n",
    "print(\"\\n3D Tensor:\")\n",
    "print(tensor_3d)\n",
    "print(\"Shape:\", tensor_3d.shape)\n",
    "print(\"Dimension:\", tensor_3d.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768827f6",
   "metadata": {},
   "source": [
    "### 2(ii) Basic Tensor Operations in PyTorch\n",
    "Perform:\n",
    "- Addition\n",
    "- Matrix multiplication\n",
    "- Dot product\n",
    "- L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2647369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor t1: tensor([1., 2., 3., 4.])\n",
      "Tensor t2: tensor([5., 6., 7., 8.])\n",
      "\n",
      "1. Addition (t1 + t2): tensor([ 6.,  8., 10., 12.])\n",
      "\n",
      "Matrix A:\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "\n",
      "Matrix B:\n",
      "tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "2. Matrix multiplication (A @ B):\n",
      "tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "3. Dot product (t1 · t2): tensor(70.)\n",
      "\n",
      "4. L2 norm of t1: tensor(5.4772)\n",
      "   L2 norm of t2: tensor(13.1909)\n"
     ]
    }
   ],
   "source": [
    "# Create tensors for operations\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "t2 = torch.tensor([5.0, 6.0, 7.0, 8.0])\n",
    "\n",
    "print(\"Tensor t1:\", t1)\n",
    "print(\"Tensor t2:\", t2)\n",
    "\n",
    "# Addition\n",
    "addition = t1 + t2  # or torch.add(t1, t2)\n",
    "print(\"\\n1. Addition (t1 + t2):\", addition)\n",
    "\n",
    "# Create matrices for matrix multiplication\n",
    "A = torch.tensor([[1.0, 2.0], \n",
    "                  [3.0, 4.0]])\n",
    "B = torch.tensor([[5.0, 6.0], \n",
    "                  [7.0, 8.0]])\n",
    "\n",
    "print(\"\\nMatrix A:\")\n",
    "print(A)\n",
    "print(\"\\nMatrix B:\")\n",
    "print(B)\n",
    "\n",
    "# Matrix multiplication\n",
    "mat_mul = torch.matmul(A, B)  # or A @ B\n",
    "print(\"\\n2. Matrix multiplication (A @ B):\")\n",
    "print(mat_mul)\n",
    "\n",
    "# Dot product\n",
    "dot_product = torch.dot(t1, t2)\n",
    "print(\"\\n3. Dot product (t1 · t2):\", dot_product)\n",
    "\n",
    "# L2 norm\n",
    "l2_norm_t1 = torch.norm(t1)\n",
    "l2_norm_t2 = torch.norm(t2)\n",
    "print(\"\\n4. L2 norm of t1:\", l2_norm_t1)\n",
    "print(\"   L2 norm of t2:\", l2_norm_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe91d6f",
   "metadata": {},
   "source": [
    "### 2(iii) Gradient Computation with Autograd\n",
    "Use PyTorch's autograd to compute gradients of a scalar-valued function automatically.\n",
    "\n",
    "For the same function: $f(x, y) = x^2 + 3y^2 + 2xy$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66584387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1.0, requires_grad = True\n",
      "y = 2.0, requires_grad = True\n",
      "\n",
      "Function f(x, y) = x² + 3y² + 2xy\n",
      "f(1.0, 2.0) = 17.0\n",
      "\n",
      "Gradients computed by autograd:\n",
      "∂f/∂x = 2x + 2y = 6.0\n",
      "∂f/∂y = 6y + 2x = 14.0\n",
      "\n",
      "Verification (analytical):\n",
      "∂f/∂x at (1, 2) = 2(1) + 2(2) = 6\n",
      "∂f/∂y at (1, 2) = 6(2) + 2(1) = 14\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with requires_grad=True to track gradients\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "print(f\"x = {x.item()}, requires_grad = {x.requires_grad}\")\n",
    "print(f\"y = {y.item()}, requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# Define the function f(x, y) = x^2 + 3y^2 + 2xy\n",
    "f = x**2 + 3*y**2 + 2*x*y\n",
    "\n",
    "print(f\"\\nFunction f(x, y) = x² + 3y² + 2xy\")\n",
    "print(f\"f({x.item()}, {y.item()}) = {f.item()}\")\n",
    "\n",
    "# Compute gradients using backward()\n",
    "f.backward()\n",
    "\n",
    "# Gradients are stored in .grad attribute\n",
    "print(f\"\\nGradients computed by autograd:\")\n",
    "print(f\"∂f/∂x = 2x + 2y = {x.grad.item()}\")\n",
    "print(f\"∂f/∂y = 6y + 2x = {y.grad.item()}\")\n",
    "\n",
    "# Verify with analytical solution\n",
    "print(f\"\\nVerification (analytical):\")\n",
    "print(f\"∂f/∂x at (1, 2) = 2(1) + 2(2) = {2*1 + 2*2}\")\n",
    "print(f\"∂f/∂y at (1, 2) = 6(2) + 2(1) = {6*2 + 2*1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930fc45",
   "metadata": {},
   "source": [
    "### 2(iv) Gradient Computation Modes Comparison\n",
    "Demonstrate the difference between:\n",
    "- `requires_grad=True` vs `requires_grad=False`\n",
    "- `torch.no_grad()` context\n",
    "- `tensor.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df98ef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. requires_grad=True vs requires_grad=False\n",
      "==================================================\n",
      "\n",
      "With requires_grad=True:\n",
      "x = 2.0, x.requires_grad = True\n",
      "y = x² = 4.0, y.requires_grad = True\n",
      "y.grad_fn = <PowBackward0 object at 0x127de71c0>\n",
      "\n",
      "With requires_grad=False:\n",
      "x = 2.0, x.requires_grad = False\n",
      "y = x² = 4.0, y.requires_grad = False\n",
      "y.grad_fn = None\n",
      "\n",
      "==================================================\n",
      "2. torch.no_grad() context\n",
      "==================================================\n",
      "\n",
      "Before no_grad: x.requires_grad = True\n",
      "Inside torch.no_grad():\n",
      "  y = x² = 9.0\n",
      "  y.requires_grad = False\n",
      "  y.grad_fn = None\n",
      "\n",
      "Outside torch.no_grad():\n",
      "  y = x² = 9.0\n",
      "  y.requires_grad = True\n",
      "  y.grad_fn = <PowBackward0 object at 0x127de71c0>\n",
      "\n",
      "==================================================\n",
      "3. tensor.detach()\n",
      "==================================================\n",
      "\n",
      "Original tensor y:\n",
      "  y = 16.0, y.requires_grad = True\n",
      "\n",
      "Detached tensor y_detached = y.detach():\n",
      "  y_detached = 16.0, y_detached.requires_grad = False\n",
      "  y_detached.grad_fn = None\n",
      "\n",
      "==================================================\n",
      "Summary:\n",
      "==================================================\n",
      "\n",
      "- requires_grad=True: Enables gradient tracking for the tensor\n",
      "- requires_grad=False: Disables gradient tracking (default for most tensors)\n",
      "- torch.no_grad(): Context manager that temporarily disables gradient computation\n",
      "                   (useful during inference to save memory and computation)\n",
      "- tensor.detach(): Creates a new tensor detached from the computation graph\n",
      "                   (shares data but won't track gradients)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. requires_grad=True vs requires_grad=False\n",
    "print(\"=\" * 50)\n",
    "print(\"1. requires_grad=True vs requires_grad=False\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# With requires_grad=True - gradient tracking enabled\n",
    "x_grad = torch.tensor(2.0, requires_grad=True)\n",
    "y_grad = x_grad ** 2\n",
    "print(f\"\\nWith requires_grad=True:\")\n",
    "print(f\"x = {x_grad.item()}, x.requires_grad = {x_grad.requires_grad}\")\n",
    "print(f\"y = x² = {y_grad.item()}, y.requires_grad = {y_grad.requires_grad}\")\n",
    "print(f\"y.grad_fn = {y_grad.grad_fn}\")\n",
    "\n",
    "# With requires_grad=False - no gradient tracking\n",
    "x_no_grad = torch.tensor(2.0, requires_grad=False)\n",
    "y_no_grad = x_no_grad ** 2\n",
    "print(f\"\\nWith requires_grad=False:\")\n",
    "print(f\"x = {x_no_grad.item()}, x.requires_grad = {x_no_grad.requires_grad}\")\n",
    "print(f\"y = x² = {y_no_grad.item()}, y.requires_grad = {y_no_grad.requires_grad}\")\n",
    "print(f\"y.grad_fn = {y_no_grad.grad_fn}\")\n",
    "\n",
    "# 2. torch.no_grad() context\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"2. torch.no_grad() context\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(f\"\\nBefore no_grad: x.requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# Inside torch.no_grad(), gradient computation is disabled\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    print(f\"Inside torch.no_grad():\")\n",
    "    print(f\"  y = x² = {y.item()}\")\n",
    "    print(f\"  y.requires_grad = {y.requires_grad}\")\n",
    "    print(f\"  y.grad_fn = {y.grad_fn}\")\n",
    "\n",
    "# Outside torch.no_grad(), gradient computation resumes\n",
    "y_outside = x ** 2\n",
    "print(f\"\\nOutside torch.no_grad():\")\n",
    "print(f\"  y = x² = {y_outside.item()}\")\n",
    "print(f\"  y.requires_grad = {y_outside.requires_grad}\")\n",
    "print(f\"  y.grad_fn = {y_outside.grad_fn}\")\n",
    "\n",
    "# 3. tensor.detach()\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"3. tensor.detach()\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "print(f\"\\nOriginal tensor y:\")\n",
    "print(f\"  y = {y.item()}, y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# detach() creates a new tensor that shares data but doesn't require gradients\n",
    "y_detached = y.detach()\n",
    "print(f\"\\nDetached tensor y_detached = y.detach():\")\n",
    "print(f\"  y_detached = {y_detached.item()}, y_detached.requires_grad = {y_detached.requires_grad}\")\n",
    "print(f\"  y_detached.grad_fn = {y_detached.grad_fn}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\"\"\n",
    "- requires_grad=True: Enables gradient tracking for the tensor\n",
    "- requires_grad=False: Disables gradient tracking (default for most tensors)\n",
    "- torch.no_grad(): Context manager that temporarily disables gradient computation\n",
    "                   (useful during inference to save memory and computation)\n",
    "- tensor.detach(): Creates a new tensor detached from the computation graph\n",
    "                   (shares data but won't track gradients)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcb3b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This assignment covered the fundamental mathematical concepts used in deep learning:\n",
    "\n",
    "**Part 1 (NumPy):**\n",
    "- Vector operations: dot product and L2 norm\n",
    "- Matrix operations: multiplication and eigenvalues\n",
    "- Calculus: partial derivatives and gradient computation for f(x,y) = x² + 3y² + 2xy\n",
    "- Probability: Shannon entropy and KL divergence\n",
    "\n",
    "**Part 2 (PyTorch):**\n",
    "- Tensor creation with different dimensions\n",
    "- Basic tensor operations (addition, matrix multiplication, dot product, L2 norm)\n",
    "- Automatic differentiation using autograd\n",
    "- Understanding gradient computation modes (requires_grad, no_grad, detach)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
